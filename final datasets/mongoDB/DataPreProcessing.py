# -*- coding: utf-8 -*-
"""DataPreProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rk2sinXPOiKfI3zv1ktBBDrzKqvrpCDF

In this notebook we clean and uniform all the 5 datasets then used for the PhoneWorld application.
The final result is composed by 3 datasets: Reviews.csv, Phones.csv and Users.csv.

All the notebook is completely executable because takes the original raw datasets directly from git.

To reload datasets, if needed, use the url of the files in the raw datasets folder at this link:
https://github.com/FrankMartoccia/phoneworld

# Settings
"""

#libraries
import pandas as pd

#set data dysplaying without truncating anything
#pd.set_option('display.max_colwidth', None)

#set data displaying options back to default
#pd.reset_option('^display.', silent=True)

"""# 20191226-items.csv"""

#load dataset
df_items = pd.read_csv('https://raw.githubusercontent.com/FrankMartoccia/phoneworld/master/raw%20datasets/20191226-items.csv?token=GHSAT0AAAAAABPN5LTAALMC44OSLQFTPLHIYPQGAYA')
df_items.head()

df_items.info()
#notice 4 null values for brand column

#control if there are any duplicates (controls if there are any rows with exactly the same values)
#if the df is empty, no row is equal to another one
df_items[df_items.duplicated()]

#try to fill the 4 rows where brand property is null
df_items[df_items['brand'].isnull()]

df_items[df_items['brand'].isnull()].url
#remember to run the snippet in settings to not truncate data values in order to have complete url link

#two brands are all caps lock, they are incompatible with brand column in df_devices
df_items.brand = df_items.brand.replace({'ASUS': 'Asus'})
df_items.brand = df_items.brand.replace({'HUAWEI': 'Huawei'})

df_items.brand[0] = 'Nokia'
df_items.brand[144] = 'Nokia' #all Lumia phones are under Nokia brand and not Microsoft
df_items.brand[471] = 'Asus'
df_items.brand[631] = 'Xiaomi'

#rename some columns to uniform language between different datasets
df_items = df_items.rename(columns={'title': 'phoneName'})

#let's see how many brands are in the dataset just showing one time each brand
df_items.brand.drop_duplicates()
#10 brands

#analyze some strange things noticed...they can be noisy data
df_items[df_items['phoneName'].str.contains("Modem")]

df_items[df_items['phoneName'].str.contains("Router")]

df_items[df_items['phoneName'].str.contains("Hotspot")]

#looking the url we notice that row 319, 555, 365, 387 are modem wifi, not smartphones so we drop them
#row 678 instead is a motorola phone so we keep it
df_items = df_items.drop(df_items.index[[319, 365, 387, 555]])

#now search for zero values in price column
df_items.originalPrice.value_counts()

#drop useless columns. Price columns are dropped because contain too many zero values.
df_items = df_items.drop(columns = ['image', 'rating', 'url', 'reviewUrl', 'totalReviews', 'price', 'originalPrice'])

"""# 20191226-reviews.csv"""

#load dataset
df_reviews = pd.read_csv('https://raw.githubusercontent.com/FrankMartoccia/phoneworld/master/raw%20datasets/20191226-reviews.csv?token=GHSAT0AAAAAABPN5LTAOQF6LLIIJDZ3HOA6YPQGBFA')
df_reviews.head()

df_reviews.info()

#control if there are any duplicates (control if there are any rows with exactly the same values)
df_reviews[df_reviews.duplicated()]

#drop the 12 duplicated rows
df_reviews = df_reviews.drop_duplicates()

#drop useless columns. Verified column is useless for our purpose.
#name column is dropped because for each review we will properly assign a fake user from the users dataset.
#helpful_votes has so much NaN values
df_reviews = df_reviews.drop(columns = ['verified', 'name', 'helpfulVotes'])

#convert to datetime the date column
df_reviews['date'] = pd.to_datetime(df_reviews['date'])

#rename some columns
df_reviews = df_reviews.rename(columns={'date': 'review_date', 'title': 'review_title', 'body': 'review_body'})

#let's see how are rows with NaN value on body and/or title
df_reviews[df_reviews['review_body'].isnull()]
#21 review_body NaN values

df_reviews[df_reviews['review_title'].isnull()]
#14 review_title NaN values

#as they are not so much data we decide to drop them
#Because we notice some review_body content repetition we can consider these as noise (probably fake reviews)
#having reviews with these uncomplete data can be a problem for any sentiment analysis or for ml alghorithms
df_reviews = df_reviews.dropna()
#it drops 30 rows
#21 with NaN body (of which 5 also without title) and 9  only without title

df_reviews.info()

df_reviews

# considerations to understand if there are fake reviews
#df_reviews.review_title[df_reviews.review_title.duplicated()]
#df_reviews.review_body[df_reviews.review_body.duplicated()]

"""# brands_csv.csv"""

#load dataset
df_brands = pd.read_csv('https://raw.githubusercontent.com/FrankMartoccia/phoneworld/master/raw%20datasets/brands_csv.csv?token=GHSAT0AAAAAABPN5LTAK3MQJFYQNZ6NGNI6YPQGBQA')
df_brands.head()

df_brands.info()

df_brands = df_brands.drop(columns = ['RECORDS/logo', 'RECORDS/deleted_at', 'RECORDS/updated_at', 'RECORDS/created_at'])

df_brands = df_brands.rename(columns = {'RECORDS/id': 'brand_id', 'RECORDS/name': 'brand'})

df_brands[df_brands.duplicated()]
#no duplicates, so there are 116 different brands.

df_brands

"""# devices_csv.csv"""

#load dataset
df_devices = pd.read_csv('https://raw.githubusercontent.com/FrankMartoccia/phoneworld/master/raw%20datasets/devices_csv.csv?token=GHSAT0AAAAAABPN5LTAN55GQNOXZJF36B4MYPQGBZQ')
df_devices.head()

df_devices.info()
#almost half of devices don't have information for the chipset, but we keep it and treat NaN values

#drop RECORDS/ on each attribute
df_devices.columns = df_devices.columns.str.replace('RECORDS/', '')

#we drop useless columns
df_devices = df_devices.drop(columns = ['id', 'url_hash', 'deleted_at','created_at', 'updated_at', 'specifications'])

#we rename columns
df_devices = df_devices.rename(columns = {'name': 'phoneName', 'body': 'dimensions', 'display_size': 'displaySize', 'display_resolution':'displayResolution', 'camera_pixels': 'cameraPixels',
                                          'video_pixels': 'videoPixels', 'battery_size':'batterySize', 'battery_type': 'batteryType' })

df_devices[df_devices.duplicated()]
#no exactly the same rows, so no duplicated rows

# NaN values in chipset and dimensions columns
df_devices = df_devices.fillna( 'NOT SPECIFIED')

df_devices.info()
#some columns are essentialy numbers but the actual type is object.
#Need to convert them and also handle with incorrect values

#finally add the phoneId to df_devices
df_devices["phoneId"] = df_devices.index + 1
df_devices

# strange values in displaySize, ram, batterySize
#df_devices[df_devices.displaySize.str.contains('&nbsp;')]
#df_devices = df_devices.replace('&nbsp;', 'NOT SPECIFIED', inplace=True)
#df_devices.ram = df_devices.ram.replace('&nbsp;', 'NO', inplace=True)
#df_devices.batterySize = df_devices.batterySize.replace('&nbsp;', 'oooooooooooooooooo')

"""# users_csv.csv"""

#load dataset
df_users = pd.read_csv('https://raw.githubusercontent.com/FrankMartoccia/phoneworld/master/raw%20datasets/users_csv.csv?token=GHSAT0AAAAAABPN5LTBH6SIGU75RJFRVRZWYPQGCCQ')
df_users.head()

df_users.info()

df_users[df_users.duplicated()]
#no duplicates, 50000 unique users

#drop results/ for all columns
df_users.columns = df_users.columns.str.replace('results/', '')

#drop useless columns
df_users = df_users.drop(columns = {'name/title', 'location/state', 'location/postcode','location/coordinates/latitude', 'location/coordinates/longitude', 'location/timezone/offset',
                                    'location/timezone/description', 'login/uuid', 'registered/date', 'registered/age', 'login/md5', 'login/sha1', 'nat'})

df_users = df_users.rename(columns = {'name/first': 'firstName', 'name/last': 'lastName', 'location/street/number': 'streetNumber', 'location/street/name': 'streetName',
                                      'location/city': 'city', 'location/country': 'country', 'login/username': 'username', 'login/password': 'password',  'login/salt': 'salt',
                                      'login/sha256': 'sha', 'dob/date': 'dateOfBirth', 'dob/age': 'age'})

df_users['dateOfBirth'] = pd.to_datetime(df_users['dateOfBirth']).dt.strftime('%Y-%m-%d')

#df_users['registered/date'] = pd.to_datetime(df_users['registered/date']).dt.strftime('%Y-%m-%d')
#il primo utente registrato è del 2002. La prima review è scritta nel 2003. 
#Volendo si potrebbe associare l'user rispettando che registered_date < review_date
#l'utente più piccolo è del 98, quindi al limite si registra all'età di 4 anni e scrive la prima recensione all'età di 5.
#soluzione: tolgo data di nascita dell'utente e tolgo la data di registrazione. Così rimane solo la data della review
#in alternativa bisogna pensare a far coesistere queste date associando per bene gli utenti alle recensioni

#create an admin column to identify the administrator user
#df_users['admin'] = ""
#df_users['admin'] = df_users['admin'].astype('bool')*1

#control if there are duplicate usernames
#because we don't want them
df_users.duplicated( subset = 'username').sum()

#so we enumerate duplicated username so to be each one unique
mask = df_users['username'].duplicated(keep=False)
df_users.loc[mask, 'username'] += df_users.groupby('username').cumcount().add(1).astype(str)

#create an _id column
df_users["_id"] = df_users.index + 1
df_users

df_users.info()

"""# Add userId and username to df_reviews



"""

#create a temporary df sample from df_users (need to execute Users_csv.csv section)
df = df_users.sample(n=70000, replace = True)
df = df.reset_index()

#ensuring _id column mantains dtype
df["_id"] = df["_id"].astype('Int64')

#add _id column to each review
df_reviews = df_reviews.join(df._id)

df_reviews

df_users

#thanks to userId we merge with df_users to add just the username
df_reviews = pd.merge(df_reviews, df_users, on = ['_id'])

#rename the _id column
df_reviews = df_reviews.rename(columns = {"_id": "userId"})

#drop useless columns for the df_reviews
df_reviews = df_reviews.drop(columns = {'gender', 'firstName', 'lastName', 'streetNumber', 'streetName', 'city', 'country',
                                        'email', 'password', 'salt', 'sha', 'dateOfBirth', 'age', })

df_reviews

"""# Add brand_id to df_items

"""

#as I need the brand_id, the df_items became this
df_items = pd.merge(df_brands, df_items)
df_items

"""# Add phoneId to df_items"""

df_items

df_devices

df_items['phoneName_new'] = ""

pd.options.mode.chained_assignment = None


for j in range(len(df_items.index)):
    nameArray = df_items['phoneName'][j].split(" ")
    if len(nameArray) >= 5:
        for i in range(5):
            if nameArray[i].endswith(","):
                nameArray[i] = nameArray[:-1]
    elif len(nameArray) >= 4:
        for i in range(4):
            if nameArray[i].endswith(","):
                nameArray[i] = nameArray[:-1]
    else:
        for i in range(3):
            if nameArray[i].endswith(","):
                nameArray[i] = nameArray[:-1]

    if len(nameArray) >= 5:
        for i in range(len(df_devices.index)):
            if (df_devices['phoneName'][i].startswith(
                f"{nameArray[0]} {nameArray[1]} {nameArray[2]} {nameArray[3]} {nameArray[4]}")):
                df_items['phoneName_new'][j] = df_devices['phoneName'][i]
    if len(nameArray) >= 4:
        for i in range(len(df_devices.index)):
            if ((df_devices['phoneName'][i].startswith(f"{nameArray[0]} {nameArray[1]} {nameArray[2]} {nameArray[3]}")
                 and len(df_items['phoneName_new'][j]) == 0)):
                df_items['phoneName_new'][j] = df_devices['phoneName'][i]
    if len(nameArray) >= 3:
        for i in range(len(df_devices.index)):
            if ((df_devices['phoneName'][i].startswith(f"{nameArray[0]} {nameArray[1]} {nameArray[2]}")
                 and len(df_items['phoneName_new'][j]) == 0)):
                print(df_devices['phoneName'][i])
                df_items['phoneName_new'][j] = df_devices['phoneName'][i]
    if len(nameArray) >= 2:
        for i in range(len(df_devices.index)):
            if ((df_devices['phoneName'][i].startswith(f"{nameArray[0]} {nameArray[1]}")
                 and len(df_items['phoneName_new'][j]) == 0)):
                df_items['phoneName_new'][j] = df_devices['phoneName'][i]
    for i in range(len(df_devices.index)):
        if ((df_devices['phoneName'][i].startswith(f"{nameArray[0]}")
            and len(df_items['phoneName_new'][j]) == 0)):
            df_items['phoneName_new'][j] = df_devices['phoneName'][i]

df_devices

df_items

#drop the old phone name column
df_items = df_items.drop(columns = {'phoneName'})

#rename the new phone name column in order to be the same as phoneName in df_devices
df_items = df_items.rename(columns = {'phoneName_new': 'phoneName'})

##finally add the phoneId to df_items_final
#merge thanks to the common columns phoneName and brand_id
df_items = pd.merge(df_items, df_devices, on = ['phoneName', 'brand_id'])
#look that from 716 here we have 687 rows
#that's right because in df_items_final there are 27 rows really without phoneName
#2 rows are dropped during the merge because brand_id and phoneName didn't match

#drop columns we don't need to merge than with df_reviews
df_items = df_items.drop(columns = {'brand_id', 'picture', 'released_at', 'dimensions', 'os', 'storage', 'displaySize', 'displayResolution', 'cameraPixels', 'videoPixels', 'ram', 'chipset', 'batterySize', 'batteryType'})

df_items

"""# Reviews

First dataset: contains all the reviews associated each one to their item, thanks to asin amazon code, taken from amazon.com
"""

df_items

df_reviews

#merge the two dataset thanks to the common column asin
Reviews = pd.merge(df_items, df_reviews)

#drop some columns in order to uniform data for the data modeling
#we don't need anymore brand attribute here
Reviews = Reviews.drop(columns = {'brand'})

#rename columns to respect data modeling indications
Reviews = Reviews.rename(columns = {'asin': '_id', 'userId': 'userId', 'review_title': 'title', 'rating': 'rating', 'review_date':'dateOfReview', 'review_body': 'body'})

#rename columns to respect data modeling indications and to have the right datatypes when uploading data on MongoDB
#Reviews = Reviews.rename(columns = {'asin': '_id.string()', 'phoneId': 'phoneId.string()', 'phoneName': 'phoneName.string()' 'userId': 'userId.string()', 'review_title': 'title.string()', 'rating': 'rating.int()', 'review_date':'dateOfReview.date()', 'review_body': 'body.string()'})

#use the _id column as index
Reviews = Reviews.set_index('_id')

Reviews
#here we have 65487 rows and not 67944
#because doing the cleaning 4 asin values are being cancelled in df_items and so reviews with that asin are not merged.
#also 29 phoneName aren't matched between df_items and df_devices 
#so we have many reviews for each product, identified by a unique asin code, phoneId and userId

#convert the df to csv format in order to be loaded on mongodb
#Reviews.to_csv('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Reviews.csv')

#Reviews.to_json('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Reviews.json')
# ValueError: DataFrame index must be unique for orient='columns'.

"""# Phones

Second dataset: contains all the smartphones taken from gsmarena.com
"""

df_brands

df_devices

#merge hanks to brand_id
Phones = pd.merge(df_brands, df_devices)

# now we don't need anymore brand_id used to merge brands and devices so we drop it
Phones = Phones.drop(columns = {'brand_id'})

#we need to take just the release year of the phone, so we take just numbers of that string
Phones.released_at = Phones.released_at.str.extract('(\d+)')

#we analyze if we collect strange value for the release date
Phones.released_at.unique()

#we view and then drop that rows with released date 2 or 20078
Phones[Phones.released_at == "2"]

Phones[Phones.released_at == "20078"]

#before running this and the next piece of code
#reload again the previous part without extracting numbers from released_at column.
#snippet useful to see the right release date
#Phones[Phones.phoneName == "Ericsson R520m"]
#code commented because is exploratory, otherwise will interrupt the run

#we assume the real release date is 2007
#df_devices[df_devices.phoneName == "BlackBerry Pearl 8130"]
#code commented because is exploratory, otherwise will interrupt the run

#adjust these two values manually
Phones['released_at'][2855] = 2001

Phones['released_at'][5087] = 2007

#drop NaN values now present in released_at column
Phones = Phones.dropna()

#rename column because it refers just to the release year
Phones = Phones.rename(columns = {'phoneId': '_id', 'released_at': 'releaseYear', 'phoneName': 'name', 'dimensions': 'body' })

#rename columns to be compliant with data modeling and specify datatype for import of csv on MongoDB
#Phones = Phones.rename(columns = {'phoneId': '_id.string()', 'brand': 'brand.string()', 'phoneName': 'name.string()', 'picture': 'picture.string()', 'releaseYear': 'releaseYear.date()', 'dimensions': 'body.string()',
#                                  'os': 'os.string()', 'storage': 'storage.string()', 'displaySize': 'displaySize.string()', 'displayResolution': 'displayResolution.string()', 'cameraPixels': 'cameraPixels.string()',
#                                  'videoPixels': 'videoPixels.string()', 'ram': 'ram.string()', 'chipset': 'chipset.string()', 'batterySize': 'batterySize.string()', 'batteryType': 'batteryType.string()'})

#use the _id column as index
Phones = Phones.set_index('_id')

Phones

#convert the df to csv format in order to be loaded on mongodb
#Phones.to_csv('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Phones.csv')

#Phones.to_json('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Phones.json')

"""# Users

Third dataset: contains all the users created from randomuser.me
"""

#take the user dataset and assign it to a new df
Users = df_users

#we dro the password for the user
Users = Users.drop(columns = {'password'})

#rename columns for data modeling compatibility and specify datatypes for MongoDB
#Users = Users.rename(columns = {'_id': '_id.string()', 'gender': 'gender.string()', 'firstName': 'firstName.string()', 'lastName': 'lastName.string()', 'streetNumber': 'streetNumber.string()',
#                                'streetName': 'streetName.string()', 'city': 'city.string()', 'country': 'country.string()', 'email': 'email.string()', 'username': 'username.string()', 'salt': 'salt.string()', 'sha': 'sha.string()',
#                                'dateOfBirth': 'dateOfBirth.date()', 'age': 'age.int()'})

#set the _id column as index
Users = Users.set_index('_id')

Users

#convert the df to csv format in order to be loaded on mongodb
#Users.to_csv('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Users.csv')

#convert the df to json format in order to be loaded on mongodb
#Users.to_json('/content/drive/MyDrive/UNIPI AIDE/Large scale and multistructured databases/Workgroup project/LSMSD group 24/Data/Users.json')